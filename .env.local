# =============================================================================
# Minute - Local Mode Configuration
# =============================================================================
# This file contains the environment variables needed to run Minute entirely
# locally with local LLM and transcription models.
#
# Copy this file to .env.local and adjust values as needed.
# 
# Usage:
#   cp .env.local.example .env.local
#   docker compose -f docker-compose.local.yaml up --build
#
# Prerequisites:
#   1. Install Ollama: https://ollama.com/download
#   2. Start Ollama: ollama serve
#   3. Pull models: ollama pull llama3.2 && ollama pull qwen2.5:32b
#   4. NVIDIA GPU with CUDA support (for optimal Whisper performance)
# =============================================================================

# === Application Settings ===
DOCKER_BUILDER_CONTAINER=minute
APP_NAME=minute
ENVIRONMENT=local
APP_URL=http://localhost:3000
BACKEND_HOST=http://localhost:8080

# === PostgreSQL Database ===
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=minute_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=insecure

# === Message Queues (LocalStack) ===
TRANSCRIPTION_QUEUE_NAME=minute-transcription-queue
TRANSCRIPTION_DEADLETTER_QUEUE_NAME=minute-transcription-queue-deadletter
LLM_QUEUE_NAME=minute-llm-queue
LLM_DEADLETTER_QUEUE_NAME=minute-llm-queue-deadletter

# === Local Storage ===
STORAGE_SERVICE_NAME=local
LOCAL_STORAGE_PATH=/static

# === Local LLM with Ollama ===
# Ollama base URL - use http://localhost:11434 when running outside Docker
# or http://host.docker.internal:11434 from within Docker containers
OLLAMA_BASE_URL=http://localhost:11434

# LLM Provider configuration - set to "ollama" for local mode
# Supported providers: "ollama", "openai", "gemini"
FAST_LLM_PROVIDER=ollama
FAST_LLM_MODEL_NAME=llama3.2

BEST_LLM_PROVIDER=ollama
# Use a larger model for best results. Options:
# - llama3.1:70b (requires ~40GB VRAM)
# - qwen2.5:32b (requires ~20GB VRAM)
# - llama3.2 (lightweight, faster but less capable)
BEST_LLM_MODEL_NAME=qwen2.5:32b

# === Local Transcription with Whisper ===
# Available services: whisper_local, azure_stt_synchronous, azure_stt_batch, aws_transcribe
TRANSCRIPTION_SERVICES=["whisper_local"]

# Whisper model size: tiny, base, small, medium, large-v2, large-v3
# Larger models are more accurate but require more VRAM/RAM
WHISPER_MODEL_SIZE=large-v3

# Device for Whisper: "cuda" for GPU (recommended), "cpu" for CPU-only
WHISPER_DEVICE=cuda

# Compute type: "float16" for GPU, "int8" for CPU, "float32" for compatibility
WHISPER_COMPUTE_TYPE=float16

# === Authentication (Disabled for Local) ===
REPO=minute
AUTH_API_URL=http://localhost:8080
DISABLE_AUTH_SIGNATURE_VERIFICATION=true

# === Azure Speech (Not used in local mode - placeholders) ===
AZURE_SPEECH_KEY=placeholder
AZURE_SPEECH_REGION=placeholder

# === Optional: Telemetry (Disabled by default) ===
SENTRY_DSN=
POSTHOG_API_KEY=

# === AWS (Not used in local mode but needed for LocalStack) ===
AWS_ACCOUNT_ID=000000000000
AWS_REGION=eu-west-2
DATA_S3_BUCKET=minute-data

# AWS credentials for LocalStack (can be fake)
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test
AWS_SESSION_TOKEN=test

# =============================================================================
# TODO: Future Enhancements
# =============================================================================
# When speaker diarization is implemented, add these settings:
#
# ENABLE_SPEAKER_DIARIZATION=false
# DIARIZATION_METHOD=resemblyzer  # Options: resemblyzer, nemo, pyannote
# 
# For pyannote.audio (best quality, requires HuggingFace token):
# HUGGINGFACE_TOKEN=hf_xxxxx
# =============================================================================
